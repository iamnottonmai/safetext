import streamlit as st
import torch
import os
import zipfile
import gdown
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Runtime paths (created dynamically)
MODEL_DIR = "model_runtime"
ZIP_PATH = "model_runtime.zip"

# Your gdown-compatible link
GDRIVE_ZIP_URL = "https://drive.google.com/uc?id=1cyjqbVRAhOAogoUWY1_zhby9uy9VdSPR"

LABELS = {
    0: "à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢",
    1: "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡"
}

@st.cache_resource
def load_model():
    if not os.path.exists(MODEL_DIR):
        with st.spinner("Downloading model from Google Drive..."):
            gdown.download(GDRIVE_ZIP_URL, ZIP_PATH, quiet=False)

        with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
            zip_ref.extractall(MODEL_DIR)

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=False   # ðŸ”¥ critical fix
    )

    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
    model.eval()

    return tokenizer, model


tokenizer, model = load_model()

# ---------------- UI ----------------

st.title("SafeText ðŸ‡¹ðŸ‡­")
st.caption("Thai Defamation & Insult Risk Analyzer")

text = st.text_area("Enter Thai text")
context = st.selectbox(
    "Context",
    ["public_post", "private_dm", "email", "letter"]
)

if st.button("Analyze") and text.strip():
    input_text = f"[CONTEXT] {context} [TEXT] {text}"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)[0]
        pred = probs.argmax().item()

    st.success(f"Prediction: **{LABELS[pred]}**")

    st.write("Confidence:")
    for i, p in enumerate(probs):
        st.write(f"- {LABELS[i]}: {p:.2%}")
