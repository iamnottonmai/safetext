import streamlit as st
import torch
import os
import zipfile
import gdown
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ---------------- CONFIG ----------------

MODEL_DIR = "model_runtime"
ZIP_PATH = "model_runtime.zip"

GDRIVE_ZIP_URL = "https://drive.google.com/uc?id=1cyjqbVRAhOAogoUWY1_zhby9uy9VdSPR"

LABELS = {
    0: "‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢",
    1: "‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á"
}

# thresholds (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
ABUSIVE_THRESHOLD = 0.20     # linguistic signal
LEGAL_THRESHOLD = 0.50       # legal risk

# ---------------- LOAD MODEL ----------------

@st.cache_resource
def load_model():
    if not os.path.exists(MODEL_DIR):
        with st.spinner("Downloading model..."):
            gdown.download(GDRIVE_ZIP_URL, ZIP_PATH, quiet=False)
            with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
                zip_ref.extractall(MODEL_DIR)

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=False
    )
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
    model.eval()

    return tokenizer, model


tokenizer, model = load_model()

# ---------------- DECISION LOGIC ----------------

def analyze(probs):
    """
    probs[1] = P(‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á)
    """
    p_risk = probs[1]

    linguistic_risk = p_risk >= ABUSIVE_THRESHOLD
    legal_risk = p_risk >= LEGAL_THRESHOLD

    if linguistic_risk and legal_risk:
        return "‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á", (
            "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡πâ‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πà‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢"
        )

    if linguistic_risk and not legal_risk:
        return "‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°", (
            "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Å‡πâ‡∏≤‡∏ß‡∏£‡πâ‡∏≤‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏π‡∏´‡∏°‡∏¥‡πà‡∏ô "
            "‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ô‡∏µ‡πâ"
        )

    if not linguistic_risk and legal_risk:
        return "‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á", (
            "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≤‡∏à‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ï‡πà‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô "
            "‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ñ‡πâ‡∏≠‡∏¢‡∏Ñ‡∏≥‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á"
        )

    return "‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢", "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ"

# ---------------- UI ----------------

st.title("SafeText üáπüá≠")
st.caption("Thai Defamation & Insult Risk Analyzer")

text = st.text_area("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
context = st.selectbox(
    "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó",
    ["public_post", "private_dm", "email", "letter"]
)

if st.button("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå") and text.strip():

    input_text = f"[CONTEXT] {context} [TEXT] {text}"

    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)[0].tolist()

    verdict, explanation = analyze(probs)

    # -------- OUTPUT --------

    if verdict in ["‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á", "‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á"]:
        st.error(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: **{verdict}**")
    elif verdict == "‡∏Ñ‡∏≥‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°":
        st.warning(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: **{verdict}**")
    else:
        st.success(f"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: **{verdict}**")

    st.write(explanation)

    st.write("üìä ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•:")
    st.write(f"- ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢: {probs[0]:.2%}")
    st.write(f"- ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {probs[1]:.2%}")
