import streamlit as st
import torch
import os
import zipfile
import gdown
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# ---------------- CONFIG ----------------

MODEL_DIR = "model_runtime"
ZIP_PATH = "model_runtime.zip"

GDRIVE_ZIP_URL = "https://drive.google.com/uc?id=1cyjqbVRAhOAogoUWY1_zhby9uy9VdSPR"

LABELS = {
    0: "à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢",
    1: "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡"
}

# à¸„à¸³à¸—à¸µà¹ˆà¸–à¸·à¸­à¸§à¹ˆà¸² "à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸—à¸²à¸‡à¸ à¸²à¸©à¸²à¹à¸šà¸šà¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸•à¸µà¸„à¸§à¸²à¸¡"
HIGH_CERTAINTY_ABUSE = {
    "à¸­à¸µà¸”à¸­à¸", "à¸­à¸µà¹€à¸«à¸µà¹‰à¸¢", "à¸­à¸µà¸ªà¸±à¸•à¸§à¹Œ", "à¸­à¸µà¸„à¸§à¸²à¸¢",
    "à¸­à¸µà¸à¸£à¸°à¸«à¸£à¸µà¹ˆ", "à¹„à¸­à¹‰à¸ªà¸±à¸•à¸§à¹Œ", "à¹„à¸­à¹‰à¸„à¸§à¸²à¸¢"
}

# ---------------- LOAD MODEL ----------------

@st.cache_resource
def load_model():
    if not os.path.exists(MODEL_DIR):
        with st.spinner("Downloading model..."):
            gdown.download(GDRIVE_ZIP_URL, ZIP_PATH, quiet=False)
            with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
                zip_ref.extractall(MODEL_DIR)

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        use_fast=False
    )
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
    model.eval()

    return tokenizer, model


tokenizer, model = load_model()

# ---------------- LOGIC ----------------

def detect_linguistic_risk(text: str):
    matched = [w for w in HIGH_CERTAINTY_ABUSE if w in text]
    return len(matched) > 0, matched


def final_decision(linguistic_risk, legal_label):
    if linguistic_risk and legal_label == "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡":
        return "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸ªà¸¹à¸‡", "à¸„à¸³à¸«à¸¢à¸²à¸š + à¸¡à¸µà¸¥à¸±à¸à¸©à¸“à¸°à¹€à¸‚à¹‰à¸²à¸‚à¹ˆà¸²à¸¢à¸à¸à¸«à¸¡à¸²à¸¢"

    if linguistic_risk and legal_label == "à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢":
        return "à¸„à¸³à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡", "à¸à¸šà¸„à¸³à¸«à¸¢à¸²à¸š à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¹€à¸‚à¹‰à¸²à¸‚à¹ˆà¸²à¸¢à¸„à¸§à¸²à¸¡à¸œà¸´à¸”à¸—à¸²à¸‡à¸à¸à¸«à¸¡à¸²à¸¢"

    if not linguistic_risk and legal_label == "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡":
        return "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¸³à¸«à¸¢à¸²à¸š à¹à¸•à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸—à¸²à¸‡à¸à¸à¸«à¸¡à¸²à¸¢"

    return "à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢", "à¹„à¸¡à¹ˆà¸à¸šà¸„à¸§à¸²à¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡"

# ---------------- UI ----------------

st.title("SafeText ğŸ‡¹ğŸ‡­")
st.caption("Thai Defamation & Insult Risk Analyzer")

text = st.text_area("à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡")
context = st.selectbox(
    "à¸šà¸£à¸´à¸šà¸—",
    ["public_post", "private_dm", "email", "letter"]
)

if st.button("à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ") and text.strip():

    # 1ï¸âƒ£ Linguistic signal
    linguistic_risk, matched_terms = detect_linguistic_risk(text)

    # 2ï¸âƒ£ Model inference
    input_text = f"[CONTEXT] {context} [TEXT] {text}"
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)[0]
        pred = probs.argmax().item()
        legal_label = LABELS[pred]

    # 3ï¸âƒ£ Final decision
    verdict, explanation = final_decision(linguistic_risk, legal_label)

    # ---------------- OUTPUT ----------------

    if verdict in ["à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡", "à¸ªà¸¸à¹ˆà¸¡à¹€à¸ªà¸µà¹ˆà¸¢à¸‡à¸ªà¸¹à¸‡"]:
        st.error(f"à¸œà¸¥à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ: **{verdict}**")
    elif verdict == "à¸„à¸³à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡":
        st.warning(f"à¸œà¸¥à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ: **{verdict}**")
    else:
        st.success(f"à¸œà¸¥à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ: **{verdict}**")

    st.write(explanation)

    if linguistic_risk:
        st.write("ğŸ” à¸•à¸£à¸§à¸ˆà¸à¸šà¸„à¸³à¹„à¸¡à¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡:", ", ".join(matched_terms))

    st.write("ğŸ“Š à¸„à¸§à¸²à¸¡à¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆà¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥:")
    for i, p in enumerate(probs):
        st.write(f"- {LABELS[i]}: {p:.2%}")
